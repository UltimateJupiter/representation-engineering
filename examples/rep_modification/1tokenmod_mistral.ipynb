{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9863aae-afda-4686-9dc2-06177ab7fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import copy\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pynvml\n",
    "\n",
    "# from repe import repe_pipeline_registry\n",
    "# repe_pipeline_registry()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    print(pynvml.nvmlDeviceGetName(handle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b908a11-a597-44da-8a44-933d3450f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False)\n",
    "tokenizer.pad_token_id = 0 \n",
    "\n",
    "user_tag = \"USER:\"\n",
    "assistant_tag = \"ASSISTANT:\"\n",
    "\n",
    "# Uncomment for first time loading the dataset\n",
    "# ds = load_dataset('tatsu-lab/alpaca', cache_dir='../../storage/cache')\n",
    "# ds.save_to_disk(\"../../storage/cache/alpaca_filtered/\")\n",
    "\n",
    "ds = load_from_disk('../../storage/cache/alpaca_filtered/')\n",
    "instructions = ds['train']['instruction']\n",
    "outputs = ds['train']['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_start = \"{user_tag} {instruction} {aug_start}\"\n",
    "template_end = \"{aug_end} {assistant_tag} {response}\"\n",
    "# Not sure why they set this offset\n",
    "cutoff_offset = 0\n",
    "\n",
    "def get_augmented_ds_tokenmod(instructions,\n",
    "                              responses,\n",
    "                              aug_start,\n",
    "                              aug_var,\n",
    "                              aug_end,\n",
    "                              max_res_len,\n",
    "                              num_examples,\n",
    "                              user_tag,\n",
    "                              assistant_tag,\n",
    "                              aug_var_length=1,\n",
    "                              ):\n",
    "    \n",
    "    ds = []\n",
    "    aug_var_tokens = tokenizer.tokenize(aug_var)\n",
    "    assert len(aug_var_tokens) == aug_var_length, aug_var_tokens\n",
    "\n",
    "    for i, (q, a) in tqdm(enumerate(zip(instructions, responses))):\n",
    "        \n",
    "        # Replaced \\n since it resulted in nontrivial peak\n",
    "        # s_tokens = tokenizer.tokenize(s)\n",
    "        a_tokens = tokenizer.tokenize(a.replace(\"\\n\", \"\"))\n",
    "\n",
    "        l_start = template_start.format(\n",
    "            user_tag=user_tag,\n",
    "            instruction=q,\n",
    "            aug_start=aug_start\n",
    "        )\n",
    "\n",
    "        l_end_base = template_end.format(\n",
    "            aug_end=aug_end,\n",
    "            assistant_tag=assistant_tag,\n",
    "            response=\"\",\n",
    "        )\n",
    "        end_token_offset = len(tokenizer.tokenize(l_end_base))\n",
    "\n",
    "        for cutoff in range(1, min(max_res_len, len(a_tokens)) - cutoff_offset):\n",
    "            a_truncated = tokenizer.convert_tokens_to_string(a_tokens[:cutoff])\n",
    "            l_end = template_end.format(\n",
    "                aug_end=aug_end,\n",
    "                assistant_tag=assistant_tag,\n",
    "                response=a_truncated,\n",
    "            )\n",
    "            neg_aug_var_offset = -end_token_offset - cutoff + 1\n",
    "            var_token_inds = list(range(neg_aug_var_offset - aug_var_length, neg_aug_var_offset))\n",
    "            l = \" \".join([l_start, aug_var, l_end])\n",
    "            ds.append([l, var_token_inds, (i, cutoff)])         \n",
    "            if len(ds) >= num_examples:\n",
    "                break\n",
    "        if len(ds) >= num_examples:\n",
    "            break\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10104it [00:01, 9004.81it/s]\n",
      "10104it [00:01, 9045.54it/s]\n"
     ]
    }
   ],
   "source": [
    "max_res_len = 2\n",
    "n_samples = 10000\n",
    "\n",
    "aug_start = \"Respond with a\"\n",
    "aug_end = \"tone.\"\n",
    "\n",
    "aug_neutral = \"normal\"\n",
    "aug_target = \"angry\"\n",
    "aug_token_len = 1\n",
    "\n",
    "neutral_ds = get_augmented_ds_tokenmod(\n",
    "    instructions,\n",
    "    outputs,\n",
    "    aug_start,\n",
    "    aug_neutral,\n",
    "    aug_end,\n",
    "    max_res_len,\n",
    "    n_samples,\n",
    "    user_tag,\n",
    "    assistant_tag,\n",
    "    aug_token_len\n",
    ")\n",
    "\n",
    "target_ds = get_augmented_ds_tokenmod(\n",
    "    instructions,\n",
    "    outputs,\n",
    "    aug_start,\n",
    "    aug_target,\n",
    "    aug_end,\n",
    "    max_res_len,\n",
    "    n_samples,\n",
    "    user_tag,\n",
    "    assistant_tag,\n",
    "    aug_token_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_reps(model, tokenizer, input_str, rep_tokens=[-1]):\n",
    "    inputs = tokenizer(input_str, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    return [m[:, rep_tokens] for m in outputs.hidden_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_embedding(seq, tokenizer, model):\n",
    "    \n",
    "    tokens = tokenizer(seq, return_tensors=\"pt\").to(model.model.device)\n",
    "    embedding = (model.model.embed_tokens(tokens['input_ids'][:, 1:]))\n",
    "    return embedding.squeeze().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapping classes for customized injection\n",
    "\n",
    "class WrappedModBlock(torch.nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.mod = None\n",
    "        self.pos = None\n",
    "        self.mask = None\n",
    "        self.mod_method = None\n",
    "\n",
    "        self.verbose = False\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "\n",
    "        output = self.block(*args, **kwargs)\n",
    "        # print(output.shape)\n",
    "        # mask = self.mask if self.mask is not None else 1.0\n",
    "\n",
    "        if self.mod is not None:\n",
    "\n",
    "            if isinstance(self.pos[0], int):\n",
    "                assert len(self.mod.shape) == 3\n",
    "                if self.mod_method == \"add\":\n",
    "                    output[:, self.pos] += self.mod\n",
    "                elif self.mod_method == \"substitute\":\n",
    "                    output[:, self.pos] = self.mod\n",
    "                else:\n",
    "                    raise NotImplementedError(\"pos dimension not implemented\")\n",
    "                # self.modify(output[:, self.pos], self.mod)\n",
    "                if self.verbose:\n",
    "                    print(\"Modifying with broadcasting: pos:{} modshape:{}\".format(self.pos, self.mod.shape))\n",
    "\n",
    "            elif isinstance(self.pos[0], list) or isinstance(self.pos[0], torch.Tensor): # assume the modification differs for different elements in the batch\n",
    "                if self.verbose:\n",
    "                    print(\"Modifying iteratively: pos:{}\".format(self.pos))\n",
    "                assert len(self.pos) == len(self.mod)\n",
    "                for i in range(len(self.pos)):\n",
    "                    if self.mod_method == \"add\":\n",
    "                        output[i, self.pos] += self.mod[i]\n",
    "                    elif self.mod_method == \"substitute\":\n",
    "                        output[i, self.pos] = self.mod[i]\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"pos dimension not implemented\")\n",
    "                    # self.modify(output[:, self.pos], self.mod)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Indexing not accepted\")\n",
    "\n",
    "        return output\n",
    "\n",
    "    def set_mod(self, mod, pos, mod_method, verbose=False):\n",
    "        self.mod = mod\n",
    "        # self.mask = masks\n",
    "        self.pos = pos\n",
    "        self.mod_method = mod_method\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def reset(self):\n",
    "        self.output = None\n",
    "        self.mod = None\n",
    "        self.pos = None\n",
    "        self.verbose = False\n",
    "\n",
    "    def set_masks(self, masks):\n",
    "        self.mask = masks\n",
    "\n",
    "    \n",
    "class WrappedModModel(torch.nn.Module):\n",
    "    def __init__(self, model, tokenizer, verbose=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.verbose = verbose\n",
    "        if verbose:\n",
    "            print(\"Creating wrapped model\")\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "        \n",
    "    def generate(self, prompt, max_new_tokens=100, random_seed=0, use_cache=True):\n",
    "        with torch.no_grad():\n",
    "            torch.random.manual_seed(random_seed)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=512, truncation=True)\n",
    "            attention_mask = inputs.attention_mask.to(self.model.device)\n",
    "            generate_ids = self.model.generate(inputs.input_ids.to(self.model.device), attention_mask=attention_mask, max_new_tokens=max_new_tokens, use_cache=use_cache)\n",
    "            return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    def get_logits(self, tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(tokens.to(self.model.device)).logits\n",
    "            return logits\n",
    "        \n",
    "    def run_prompt(self, prompt, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=512, truncation=True)\n",
    "            input_ids = inputs.input_ids.to(self.model.device)\n",
    "            attention_mask = inputs.attention_mask.to(self.model.device)\n",
    "            output = self.model(input_ids, attention_mask=attention_mask)\n",
    "            return output\n",
    "        \n",
    "    def wrap_decoder_block(self, layer_id):\n",
    "        block = self.model.model.layers[layer_id]\n",
    "        if not self.is_wrapped(block):\n",
    "            self.model.model.layers[layer_id] = WrappedModBlock(block)\n",
    "    \n",
    "    def wrap_embedding_block(self):\n",
    "        block = self.model.model.embed_tokens\n",
    "        if not self.is_wrapped(block):\n",
    "            self.model.model.embed_tokens = WrappedModBlock(block)\n",
    "    \n",
    "    def wrap_all(self):\n",
    "        for layer_id, layer in enumerate(self.model.model.layers):\n",
    "            self.wrap_decoder_block(layer_id)\n",
    "            \n",
    "    def wrap_block(self, layer_ids):\n",
    "        def _wrap_block(layer_id):\n",
    "            self.wrap_decoder_block(layer_id)\n",
    "        if isinstance(layer_ids, list) or isinstance(layer_ids, tuple) or isinstance(layer_ids, np.ndarray):\n",
    "            for layer_id in layer_ids:\n",
    "                _wrap_block(layer_id)\n",
    "        else:\n",
    "            _wrap_block(layer_ids)\n",
    "    \n",
    "    def wrap_embedding_block(self):\n",
    "        block = self.model.model.embed_tokens\n",
    "        if not self.is_wrapped(block):\n",
    "            self.model.model.embed_tokens = WrappedModBlock(block)\n",
    "\n",
    "    def get_activations(self, layer_ids):\n",
    "\n",
    "        def _get_activations(layer_id):\n",
    "            current_layer = self.model.model.layers[layer_id]\n",
    "\n",
    "            if self.is_wrapped(current_layer):\n",
    "                current_block = current_layer.block\n",
    "                return current_layer.output\n",
    "                \n",
    "        if isinstance(layer_ids, list) or isinstance(layer_ids, tuple) or isinstance(layer_ids, np.ndarray):\n",
    "            activations = {}\n",
    "            for layer_id in layer_ids:\n",
    "                activations[layer_id] = _get_activations(layer_id)\n",
    "            return activations\n",
    "        else:\n",
    "            return _get_activations(layer_ids)\n",
    "\n",
    "    def set_mod(self, layer_ids, activations, block_name='decoder_block', pos=None, masks=None, normalize=False):\n",
    "\n",
    "        def _set_mod(layer_id, activations, block_name, masks, normalize):\n",
    "            current_layer = self.model.model.layers[layer_id]\n",
    "            current_layer.set_mod(activations, pos, masks, normalize)\n",
    "                \n",
    "        if isinstance(layer_ids, list) or isinstance(layer_ids, tuple) or isinstance(layer_ids, np.ndarray):\n",
    "            assert isinstance(activations, dict), \"activations should be a dictionary\"\n",
    "            for layer_id in layer_ids:\n",
    "                _set_mod(layer_id, activations[layer_id], block_name, masks, normalize)\n",
    "        else:\n",
    "            _set_mod(layer_ids, activations, block_name, masks, normalize)\n",
    "    \n",
    "    def set_mod_embedding(self, mod, pos, mod_method=\"substitute\"):\n",
    "        block = self.model.model.embed_tokens\n",
    "        assert self.is_wrapped(block)\n",
    "        block.set_mod(mod, pos, mod_method, verbose=self.verbose)\n",
    "        if self.verbose:\n",
    "            print(\"Setting embedding modification\")\n",
    "\n",
    "    def reset(self):\n",
    "        for layer in self.model.model.layers:\n",
    "            if self.is_wrapped(layer):\n",
    "                layer.reset()\n",
    "        self.model.model.embed_tokens.reset()\n",
    "\n",
    "    def set_masks(self, masks):\n",
    "        for layer in self.model.model.layers:\n",
    "            if self.is_wrapped(layer):\n",
    "                layer.set_masks(masks)\n",
    "\n",
    "    def is_wrapped(self, block):\n",
    "        if hasattr(block, 'block'):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def unwrap(self):\n",
    "        for l, layer in enumerate(self.model.model.layers):\n",
    "            if self.is_wrapped(layer):\n",
    "                self.model.model.layers[l] = layer.block\n",
    "        if self.is_wrapped(self.model.model.embed_tokens):\n",
    "            self.model.model.embed_tokens = model.model.embed_tokens.block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0051, -0.0053,  0.0044,  ..., -0.0010, -0.0022,  0.0003],\n",
      "         [-0.0051, -0.0053,  0.0044,  ..., -0.0010, -0.0022,  0.0003],\n",
      "         [-0.0051, -0.0053,  0.0044,  ..., -0.0010, -0.0022,  0.0003]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wrapped_model = WrappedModModel(model, tokenizer, verbose=False)\n",
    "wrapped_model.unwrap()\n",
    "wrapped_model.wrap_embedding_block()\n",
    "test_tokens = tokenizer(neutral_ds[0][0], return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "# wrapped_model(**test_tokens)\n",
    "init_embedding = get_seq_embedding(\"_ _ _\", tokenizer, model).unsqueeze(0)\n",
    "test_pos = [-1, -2, -3]\n",
    "\n",
    "wrapped_model.set_mod_embedding(init_embedding, test_pos)\n",
    "wrapped_model(**test_tokens)\n",
    "# get_hidden_reps(wrapped_model, tokenizer, neutral_ds[0][0])\n",
    "\n",
    "# wrapped_model.reset()\n",
    "get_hidden_reps(wrapped_model, tokenizer, neutral_ds[0][0], rep_tokens=[-1, -2])\n",
    "print(init_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_reps(wrapped_model: WrappedModModel, tokenizer, input_strs, sample_ids, mod, mod_pos, rep_tokens, grad_comp=True):\n",
    "    # Return hidden representation for (modified) inputs\n",
    "\n",
    "    wrapped_model.reset()\n",
    "    wrapped_model.model.zero_grad()\n",
    "    if mod is not None:\n",
    "        wrapped_model.set_mod_embedding(mod, mod_pos)\n",
    "    \n",
    "    tokenized_inputs = tokenizer(input_strs, return_tensors=\"pt\", padding=True).to(wrapped_model.model.device)\n",
    "    if not grad_comp:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_inputs, output_hidden_states=True)\n",
    "    else:\n",
    "        outputs = model(**tokenized_inputs, output_hidden_states=True)\n",
    "    reps = [m[:, rep_tokens] for m in outputs.hidden_states]\n",
    "    assert len(sample_ids) == reps[0].shape[0], \"sample_id passed incorrectly\"\n",
    "    \n",
    "    res = {sample_id: [x[i] for x in reps] for i, sample_id in enumerate(sample_ids)}\n",
    "    return res, outputs\n",
    "\n",
    "def comp_target_reps_interpolation(reps_init, reps_dest, dest_weight):\n",
    "    # Interpolate between reps_init and reps_dest\n",
    "    res = {}\n",
    "    for k in reps_init.keys():\n",
    "        l = []\n",
    "        for i in range(len(reps_init[k])):\n",
    "            l.append((reps_init[k][i] * (1 - dest_weight) + reps_dest[k][i] * dest_weight).detach())\n",
    "            # print(torch.norm(reps_init[k][i] - reps_dest[k][i]))\n",
    "        res[k] = l\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_rep_diff(base, target, layer_ids):\n",
    "    loss = 0\n",
    "    for l in layer_ids:\n",
    "        # print(base[l], target[l])\n",
    "        loss += torch.norm(base[l] - target[l])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 32/1250 [00:06<04:09,  4.89it/s, Loss: 168.0000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m             loss_record\u001b[39m.\u001b[39mappend(total_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mitem())\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_record, var_embedding\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m res \u001b[39m=\u001b[39m train_contrastive(model, tokenizer, target_ds, aug_neutral, aug_target, epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, batchsize\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, rep_tokens\u001b[39m=\u001b[39m[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], rep_layers\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m18\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m5\u001b[39m)), coef\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, caching_base\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m var_embedding_dups:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     x\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m reps_var, all_reps \u001b[39m=\u001b[39m comp_reps(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     wrapped_model,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m     tokenizer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m     input_strs,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m     sample_id_ls,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     mod\u001b[39m=\u001b[39mvar_embedding_dups,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     mod_pos\u001b[39m=\u001b[39mmod_pos_ls,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     rep_tokens\u001b[39m=\u001b[39mrep_tokens,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m     grad_comp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39m# print(var_embedding_dups)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39m# Compute representation difference in l2 metric\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m losses \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;32m/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenized_inputs, output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenized_inputs, output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m reps \u001b[39m=\u001b[39m [m[:, rep_tokens] \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m outputs\u001b[39m.\u001b[39mhidden_states]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225072696e6365746f6e4164726f69744a756d70227d/home/xz4134/Research/representation-engineering/examples/rep_modification/1tokenmod_mistral.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(sample_ids) \u001b[39m==\u001b[39m reps[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39msample_id passed incorrectly\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1009\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1006\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1008\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m   1010\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1011\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1012\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   1013\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1014\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1015\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1016\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1017\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1018\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[1;32m   1021\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1022\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:897\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    887\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    888\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    889\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m         use_cache,\n\u001b[1;32m    895\u001b[0m     )\n\u001b[1;32m    896\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 897\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    898\u001b[0m         hidden_states,\n\u001b[1;32m    899\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    900\u001b[0m         position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m    901\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[1;32m    902\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    903\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    904\u001b[0m     )\n\u001b[1;32m    906\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    908\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:626\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    625\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 626\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    627\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    628\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    629\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m    630\u001b[0m     past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[1;32m    631\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    632\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    633\u001b[0m )\n\u001b[1;32m    634\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    636\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:298\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    296\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mreshape(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\n\u001b[0;32m--> 298\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mo_proj(attn_output)\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    301\u001b[0m     attn_weights \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/network/xz4134/miniconda/miniconda_envs/repe/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_contrastive(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    ds_dest,\n",
    "    init_str,\n",
    "    target_str,\n",
    "    epochs,\n",
    "    batchsize,\n",
    "    rep_tokens,\n",
    "    rep_layers,\n",
    "    coef=1,\n",
    "    caching_base=True,\n",
    "    pbar=True\n",
    "):\n",
    "\n",
    "    cached_target_reps = {}\n",
    "    loss_record = []\n",
    "\n",
    "    # Created wrapped model for injection\n",
    "    wrapped_model = WrappedModModel(model, tokenizer, verbose=False)\n",
    "    wrapped_model.unwrap()\n",
    "    wrapped_model.wrap_embedding_block()\n",
    "    wrapped_model.reset()\n",
    "\n",
    "    embedding_init = get_seq_embedding(init_str, tokenizer, wrapped_model.model)\n",
    "    # print(embedding_init)\n",
    "\n",
    "    var_embedding = embedding_init.clone().float()\n",
    "    optimizer = torch.optim.SGD([var_embedding], lr=0.001)\n",
    "    # var_embedding.requires_grad = True\n",
    "\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        batch_range = range((len(ds_dest) + batchsize - 1) // batchsize)\n",
    "        if pbar:\n",
    "            batch_range = tqdm(batch_range, desc=f\"Epoch {i}\")\n",
    "            \n",
    "        for batch_ind in batch_range:\n",
    "\n",
    "            # if device != \"cpu\":\n",
    "            #     info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            #     print(info.free / info.total)\n",
    "            \n",
    "            # Compute or retrive target representation\n",
    "            sample_id_ls = [] # all sample ids\n",
    "            sample_id_ls_target_recomp = [] # ids not found in cache\n",
    "\n",
    "            input_strs, mod_pos_ls = [], []\n",
    "            input_strs_target_recomp, mod_pos_ls_target_recomp = [], []\n",
    "            \n",
    "            for target_str, mod_pos, sample_id in ds_dest[batch_ind * batchsize : (batch_ind + 1) * batchsize]:\n",
    "                \n",
    "                # sample_id: (i, j) - the j-th truncation for the i-th sentence\n",
    "                sample_id_ls.append(sample_id)\n",
    "                input_strs.append(target_str)\n",
    "                mod_pos_ls.append(mod_pos)\n",
    "                \n",
    "                if sample_id not in cached_target_reps:\n",
    "                    sample_id_ls_target_recomp.append(sample_id)\n",
    "                    input_strs_target_recomp.append(target_str)\n",
    "                    mod_pos_ls_target_recomp.append(mod_pos)\n",
    "                \n",
    "            # Compute reps at init and target token\n",
    "            if len(sample_id_ls_target_recomp) != 0:\n",
    "                # Recompute\n",
    "                reps_init_recomp, _ = comp_reps(\n",
    "                    wrapped_model,\n",
    "                    tokenizer,\n",
    "                    input_strs_target_recomp,\n",
    "                    sample_id_ls_target_recomp,\n",
    "                    mod=[embedding_init.clone() for i in range(len(sample_id_ls_target_recomp))],\n",
    "                    mod_pos=mod_pos_ls_target_recomp,\n",
    "                    rep_tokens=rep_tokens,\n",
    "                    grad_comp=False\n",
    "                )\n",
    "\n",
    "                reps_dest_recomp, _ = comp_reps(\n",
    "                    wrapped_model,\n",
    "                    tokenizer,\n",
    "                    input_strs_target_recomp,\n",
    "                    sample_id_ls_target_recomp,\n",
    "                    mod=None,\n",
    "                    mod_pos=None,\n",
    "                    rep_tokens=rep_tokens,\n",
    "                    grad_comp=False\n",
    "                )\n",
    "\n",
    "                reps_target_recomp = comp_target_reps_interpolation(reps_init_recomp, reps_dest_recomp, coef)\n",
    "                cached_target_reps.update(reps_target_recomp)\n",
    "\n",
    "            reps_target = {sample_id: cached_target_reps[sample_id] for sample_id in sample_id_ls}\n",
    "            # Compute the representation with the varying embedding\n",
    "\n",
    "            # Create the varying embedding representations\n",
    "            var_embedding_dups = [var_embedding.clone().half() for i in range(len(sample_id_ls))]\n",
    "            for x in var_embedding_dups:\n",
    "                x.requires_grad = True\n",
    "\n",
    "            reps_var, all_reps = comp_reps(\n",
    "                wrapped_model,\n",
    "                tokenizer,\n",
    "                input_strs,\n",
    "                sample_id_ls,\n",
    "                mod=var_embedding_dups,\n",
    "                mod_pos=mod_pos_ls,\n",
    "                rep_tokens=rep_tokens,\n",
    "                grad_comp=True\n",
    "            )\n",
    "            # print(var_embedding_dups)\n",
    "\n",
    "            # Compute representation difference in l2 metric\n",
    "            losses = {}\n",
    "            for sample_id in reps_var:\n",
    "                losses[sample_id] = l2_rep_diff(reps_var[sample_id], reps_target[sample_id], rep_layers)\n",
    "            \n",
    "            # Back prop the loss to modified embeddings\n",
    "            total_loss = 0\n",
    "            for sample_id in losses:\n",
    "                total_loss += losses[sample_id]\n",
    "            total_loss.backward()\n",
    "            del all_reps\n",
    "            \n",
    "            batch_range.set_postfix_str(\"Loss: {:.4f}\".format(total_loss.detach().item()))\n",
    "            \n",
    "            # for i in range(len(var_embedding_dups)):\n",
    "            #     print(i, var_embedding_dups[i].grad)\n",
    "\n",
    "            # tokenized_var_strs = tokenizer(var_strs, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "            # print(model(**tokenized_var_strs))\n",
    "            optimizer.zero_grad()\n",
    "            ave_grad = torch.mean(torch.stack([var_embedding_dups[i].grad for i in range(len(var_embedding_dups))]), dim=0)\n",
    "            var_embedding.grad = ave_grad.float()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            wrapped_model.model.model.zero_grad()\n",
    "            for x in var_embedding_dups:\n",
    "                del x\n",
    "            \n",
    "            if not caching_base:\n",
    "                cached_target_reps = {}\n",
    "            \n",
    "            loss_record.append(total_loss.detach().item())\n",
    "\n",
    "    return loss_record, var_embedding\n",
    "res = train_contrastive(model, tokenizer, target_ds, aug_neutral, aug_target, epochs=5, batchsize=8, rep_tokens=[-1], rep_layers=list(range(-18, -5)), coef=1, caching_base=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14fee0961ad0>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(moving_average(res[0], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000],\n",
      "        [-0.0253],\n",
      "        [ 0.0117],\n",
      "        ...,\n",
      "        [-0.0121],\n",
      "        [ 0.0085],\n",
      "        [ 0.0116]], device='cuda:0')\n",
      "tensor(0.0652, device='cuda:0') blind\n",
      "tensor(0.0652, device='cuda:0') heated\n",
      "tensor(0.0653, device='cuda:0') smiling\n",
      "tensor(0.0653, device='cuda:0') frustration\n",
      "tensor(0.0655, device='cuda:0') blame\n",
      "tensor(0.0656, device='cuda:0') threats\n",
      "tensor(0.0659, device='cuda:0') defending\n",
      "tensor(0.0661, device='cuda:0') armed\n",
      "tensor(0.0662, device='cuda:0') Random\n",
      "tensor(0.0663, device='cuda:0') razy\n",
      "tensor(0.0663, device='cuda:0') phys\n",
      "tensor(0.0664, device='cuda:0') heavy\n",
      "tensor(0.0669, device='cuda:0') gency\n",
      "tensor(0.0672, device='cuda:0') dispute\n",
      "tensor(0.0676, device='cuda:0') smart\n",
      "tensor(0.0677, device='cuda:0') insult\n",
      "tensor(0.0678, device='cuda:0') lust\n",
      "tensor(0.0681, device='cuda:0') elder\n",
      "tensor(0.0682, device='cuda:0') shocked\n",
      "tensor(0.0684, device='cuda:0') ruined\n",
      "tensor(0.0685, device='cuda:0') required\n",
      "tensor(0.0685, device='cuda:0') genuinely\n",
      "tensor(0.0689, device='cuda:0') threatened\n",
      "tensor(0.0690, device='cuda:0') severely\n",
      "tensor(0.0690, device='cuda:0') desperate\n",
      "tensor(0.0691, device='cuda:0') deadly\n",
      "tensor(0.0692, device='cuda:0') emotional\n",
      "tensor(0.0695, device='cuda:0') funny\n",
      "tensor(0.0696, device='cuda:0') disg\n",
      "tensor(0.0702, device='cuda:0') revenge\n",
      "tensor(0.0703, device='cuda:0') critical\n",
      "tensor(0.0706, device='cuda:0') msg\n",
      "tensor(0.0707, device='cuda:0') surprised\n",
      "tensor(0.0708, device='cuda:0') scary\n",
      "tensor(0.0710, device='cuda:0') rally\n",
      "tensor(0.0713, device='cuda:0') sexy\n",
      "tensor(0.0714, device='cuda:0') sadly\n",
      "tensor(0.0714, device='cuda:0') harmful\n",
      "tensor(0.0714, device='cuda:0') temper\n",
      "tensor(0.0716, device='cuda:0') sick\n",
      "tensor(0.0717, device='cuda:0') older\n",
      "tensor(0.0722, device='cuda:0') dark\n",
      "tensor(0.0725, device='cuda:0') rejected\n",
      "tensor(0.0726, device='cuda:0') negative\n",
      "tensor(0.0728, device='cuda:0') laughing\n",
      "tensor(0.0729, device='cuda:0') scared\n",
      "tensor(0.0731, device='cuda:0') urbed\n",
      "tensor(0.0732, device='cuda:0') confused\n",
      "tensor(0.0737, device='cuda:0') crying\n",
      "tensor(0.0739, device='cuda:0') math\n",
      "tensor(0.0739, device='cuda:0') hatred\n",
      "tensor(0.0742, device='cuda:0') defensive\n",
      "tensor(0.0743, device='cuda:0') proud\n",
      "tensor(0.0746, device='cuda:0') racist\n",
      "tensor(0.0747, device='cuda:0') lonely\n",
      "tensor(0.0750, device='cuda:0') shitty\n",
      "tensor(0.0750, device='cuda:0') reconc\n",
      "tensor(0.0756, device='cuda:0') evil\n",
      "tensor(0.0756, device='cuda:0') injured\n",
      "tensor(0.0760, device='cuda:0') wounded\n",
      "tensor(0.0764, device='cuda:0') happy\n",
      "tensor(0.0774, device='cuda:0') arguing\n",
      "tensor(0.0787, device='cuda:0') cold\n",
      "tensor(0.0789, device='cuda:0') rich\n",
      "tensor(0.0792, device='cuda:0') hate\n",
      "tensor(0.0793, device='cuda:0') acc\n",
      "tensor(0.0796, device='cuda:0') awful\n",
      "tensor(0.0796, device='cuda:0') harsh\n",
      "tensor(0.0799, device='cuda:0') stiff\n",
      "tensor(0.0806, device='cuda:0') wealthy\n",
      "tensor(0.0806, device='cuda:0') threatening\n",
      "tensor(0.0810, device='cuda:0') loyal\n",
      "tensor(0.0812, device='cuda:0') frightened\n",
      "tensor(0.0817, device='cuda:0') aggressive\n",
      "tensor(0.0822, device='cuda:0') sad\n",
      "tensor(0.0823, device='cuda:0') yelled\n",
      "tensor(0.0835, device='cuda:0') jealous\n",
      "tensor(0.0843, device='cuda:0') tense\n",
      "tensor(0.0843, device='cuda:0') violent\n",
      "tensor(0.0848, device='cuda:0') worried\n",
      "tensor(0.0852, device='cuda:0') nasty\n",
      "tensor(0.0853, device='cuda:0') fierce\n",
      "tensor(0.0865, device='cuda:0') concerned\n",
      "tensor(0.0867, device='cuda:0') rude\n",
      "tensor(0.0882, device='cuda:0') annoy\n",
      "tensor(0.0884, device='cuda:0') outrage\n",
      "tensor(0.0890, device='cuda:0') bitter\n",
      "tensor(0.0892, device='cuda:0') unhappy\n",
      "tensor(0.0912, device='cuda:0') drunk\n",
      "tensor(0.0925, device='cuda:0') frustrated\n",
      "tensor(0.0931, device='cuda:0') 😡\n",
      "tensor(0.0942, device='cuda:0') anxious\n",
      "tensor(0.0949, device='cuda:0') hungry\n",
      "tensor(0.0950, device='cuda:0') ugly\n",
      "tensor(0.0959, device='cuda:0') hostile\n",
      "tensor(0.1085, device='cuda:0') rage\n",
      "tensor(0.1190, device='cuda:0') mad\n",
      "tensor(0.1363, device='cuda:0') upset\n",
      "tensor(0.1439, device='cuda:0') anger\n",
      "tensor(0.2600, device='cuda:0') angry\n"
     ]
    }
   ],
   "source": [
    "v_trained = res[1]\n",
    "embedding = model.model.embed_tokens.block.weight.detach().float()\n",
    "diff = embedding.matmul(v_trained.unsqueeze(1))\n",
    "\n",
    "print(diff)\n",
    "for k in diff.flatten().argsort()[-100:]:\n",
    "    print(diff[k].norm(), tokenizer.decode(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "préc\n",
      "deutschen\n",
      "ricane\n",
      "hausen\n",
      "unnecessary\n",
      "openly\n",
      "breeze\n",
      "predomin\n",
      "lonely\n",
      "countryside\n",
      "sometime\n",
      "Bronnen\n",
      "keit\n",
      "jew\n",
      "adget\n",
      "cknow\n",
      "Haupt\n",
      "accomp\n",
      "wright\n",
      "_,\n",
      "Sounds\n",
      "beard\n",
      "bunch\n",
      "famille\n",
      "adays\n",
      "simplicity\n",
      "Arkansas\n",
      "unc\n",
      "Serializer\n",
      "Johannes\n",
      "eyeb\n",
      "compelling\n",
      "inceton\n",
      "hbar\n",
      "Illuminate\n",
      "succeeded\n",
      "př\n",
      "konnte\n",
      "utterly\n",
      "exclusively\n",
      "dés\n",
      "equipped\n",
      "lieutenant\n",
      "displaystyle\n",
      "км\n",
      "discour\n",
      "пів\n",
      "комп\n",
      "ucket\n",
      "remarkable\n",
      "TestCase\n",
      "entstand\n",
      "wikipedia\n",
      "ahoo\n",
      "deutscher\n",
      "migrations\n",
      "itzerland\n",
      "Sic\n",
      "bezeichnet\n",
      "instanceof\n",
      "Quebec\n",
      "renown\n",
      "участи\n",
      "erhielt\n",
      "département\n",
      "altogether\n",
      "underarter\n",
      "homework\n",
      "getElementById\n",
      "aille\n",
      "ubernetes\n",
      "forehead\n",
      "forEach\n",
      "listade\n",
      "youtube\n",
      "Buenos\n",
      "thereby\n",
      "preventDefault\n",
      "gemeente\n",
      "ˈ\n",
      "Philipp\n",
      "Universität\n",
      "ww\n",
      "keine\n",
      "irrelevant\n",
      "toire\n",
      "Budapest\n",
      "pione\n",
      "Одна\n",
      "afterward\n",
      "onClick\n",
      "jewelry\n",
      "phenomenon\n",
      "😅\n",
      "przez\n",
      "Suppress\n",
      "￼\n",
      "TextField\n",
      "latach\n",
      "släktet\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'släktet'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(12645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_trained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8511, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_trained.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  14340 MiB |  29047 MiB |  28681 GiB |  28667 GiB |\n",
      "|       from large pool |  14340 MiB |  29037 MiB |  28389 GiB |  28375 GiB |\n",
      "|       from small pool |      0 MiB |    186 MiB |    291 GiB |    291 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  14340 MiB |  29047 MiB |  28681 GiB |  28667 GiB |\n",
      "|       from large pool |  14340 MiB |  29037 MiB |  28389 GiB |  28375 GiB |\n",
      "|       from small pool |      0 MiB |    186 MiB |    291 GiB |    291 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  14340 MiB |  29021 MiB |  28337 GiB |  28323 GiB |\n",
      "|       from large pool |  14340 MiB |  29012 MiB |  28045 GiB |  28031 GiB |\n",
      "|       from small pool |      0 MiB |    186 MiB |    291 GiB |    291 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  14636 MiB |  37110 MiB |  37112 MiB |  22476 MiB |\n",
      "|       from large pool |  14620 MiB |  36916 MiB |  36916 MiB |  22296 MiB |\n",
      "|       from small pool |     16 MiB |    194 MiB |    196 MiB |    180 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 302159 KiB |   6724 MiB |  21948 GiB |  21947 GiB |\n",
      "|       from large pool | 286464 KiB |   6718 MiB |  21643 GiB |  21643 GiB |\n",
      "|       from small pool |  15695 KiB |    146 MiB |    304 GiB |    304 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     408    |    1898    |    5152 K  |    5152 K  |\n",
      "|       from large pool |     292    |    1143    |    3333 K  |    3333 K  |\n",
      "|       from small pool |     116    |     974    |    1819 K  |    1819 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     408    |    1898    |    5152 K  |    5152 K  |\n",
      "|       from large pool |     292    |    1143    |    3333 K  |    3333 K  |\n",
      "|       from small pool |     116    |     974    |    1819 K  |    1819 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     236    |     621    |     622    |     386    |\n",
      "|       from large pool |     228    |     524    |     524    |     296    |\n",
      "|       from small pool |       8    |      97    |      98    |      90    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      87    |     400    |    3164 K  |    3164 K  |\n",
      "|       from large pool |      67    |     335    |    2158 K  |    2158 K  |\n",
      "|       from small pool |      20    |     219    |    1005 K  |    1005 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
