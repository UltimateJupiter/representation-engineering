{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9863aae-afda-4686-9dc2-06177ab7fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xz4134/storage/miniconda3/envs/repe/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import copy\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pynvml\n",
    "\n",
    "# from repe import repe_pipeline_registry\n",
    "# repe_pipeline_registry()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    print(pynvml.nvmlDeviceGetName(handle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b908a11-a597-44da-8a44-933d3450f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.71s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False)\n",
    "tokenizer.pad_token_id = 0 \n",
    "\n",
    "user_tag = \"USER:\"\n",
    "assistant_tag = \"ASSISTANT:\"\n",
    "\n",
    "# Uncomment for first time loading the dataset\n",
    "# ds = load_dataset('tatsu-lab/alpaca', cache_dir='../../storage/cache')\n",
    "# ds.save_to_disk(\"../../storage/cache/alpaca_filtered/\")\n",
    "\n",
    "ds = load_from_disk('../../storage/cache/alpaca_filtered/')\n",
    "instructions = ds['train']['instruction']\n",
    "outputs = ds['train']['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_start = \"{user_tag} {instruction} {aug_start}\"\n",
    "template_end = \"{aug_end} {assistant_tag} {response}\"\n",
    "# Not sure why they set this offset\n",
    "cutoff_offset = 0\n",
    "\n",
    "def get_augmented_ds_tokenmod(instructions,\n",
    "                              responses,\n",
    "                              aug_start,\n",
    "                              aug_var,\n",
    "                              aug_end,\n",
    "                              max_res_len,\n",
    "                              num_examples,\n",
    "                              user_tag,\n",
    "                              assistant_tag,\n",
    "                              aug_var_length=1,\n",
    "                              ):\n",
    "    \n",
    "    ds = []\n",
    "    aug_var_tokens = tokenizer.tokenize(aug_var)\n",
    "    assert len(aug_var_tokens) == aug_var_length, aug_var_tokens\n",
    "\n",
    "    for i, (q, a) in tqdm(enumerate(zip(instructions, responses))):\n",
    "        \n",
    "        # Replaced \\n since it resulted in nontrivial peak\n",
    "        # s_tokens = tokenizer.tokenize(s)\n",
    "        a_tokens = tokenizer.tokenize(a.replace(\"\\n\", \"\"))\n",
    "\n",
    "        l_start = template_start.format(\n",
    "            user_tag=user_tag,\n",
    "            instruction=q,\n",
    "            aug_start=aug_start\n",
    "        )\n",
    "\n",
    "        l_end_base = template_end.format(\n",
    "            aug_end=aug_end,\n",
    "            assistant_tag=assistant_tag,\n",
    "            response=\"\",\n",
    "        )\n",
    "        end_token_offset = len(tokenizer.tokenize(l_end_base))\n",
    "\n",
    "        for cutoff in range(1, min(max_res_len, len(a_tokens)) - cutoff_offset):\n",
    "            a_truncated = tokenizer.convert_tokens_to_string(a_tokens[:cutoff])\n",
    "            l_end = template_end.format(\n",
    "                aug_end=aug_end,\n",
    "                assistant_tag=assistant_tag,\n",
    "                response=a_truncated,\n",
    "            )\n",
    "            neg_aug_var_offset = -end_token_offset - cutoff + 1\n",
    "            var_token_inds = list(range(neg_aug_var_offset - aug_var_length, neg_aug_var_offset))\n",
    "            l = \" \".join([l_start, aug_var, l_end])\n",
    "            ds.append([l, var_token_inds, (i, cutoff)])         \n",
    "            if len(ds) >= num_examples:\n",
    "                break\n",
    "        if len(ds) >= num_examples:\n",
    "            break\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 638.66it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 706.12it/s]\n"
     ]
    }
   ],
   "source": [
    "max_res_len = 128\n",
    "n_samples = 1000\n",
    "\n",
    "aug_start = \"Respond with a\"\n",
    "aug_end = \"tone.\"\n",
    "\n",
    "aug_neutral = \"_ _ _\"\n",
    "aug_target = \"_ _ angry\"\n",
    "aug_token_len = 3\n",
    "\n",
    "neutral_ds = get_augmented_ds_tokenmod(\n",
    "    instructions,\n",
    "    outputs,\n",
    "    aug_start,\n",
    "    aug_neutral,\n",
    "    aug_end,\n",
    "    max_res_len,\n",
    "    n_samples,\n",
    "    user_tag,\n",
    "    assistant_tag,\n",
    "    aug_token_len\n",
    ")\n",
    "\n",
    "target_ds = get_augmented_ds_tokenmod(\n",
    "    instructions,\n",
    "    outputs,\n",
    "    aug_start,\n",
    "    aug_target,\n",
    "    aug_end,\n",
    "    max_res_len,\n",
    "    n_samples,\n",
    "    user_tag,\n",
    "    assistant_tag,\n",
    "    aug_token_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_reps(model, tokenizer, input_str, rep_tokens=[-1]):\n",
    "    inputs = tokenizer(input_str, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    return [m[:, rep_tokens] for m in outputs.hidden_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_embedding(seq, tokenizer, model):\n",
    "    \n",
    "    tokens = tokenizer(seq, return_tensors=\"pt\").to(model.model.device)\n",
    "    embedding = (model.model.embed_tokens(tokens['input_ids'][:, 1:]))\n",
    "    return embedding.squeeze().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapping classes for customized injection\n",
    "\n",
    "class WrappedModBlock(torch.nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.mod = None\n",
    "        self.pos = None\n",
    "        self.mask = None\n",
    "        self.mod_method = None\n",
    "\n",
    "        self.verbose = False\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "\n",
    "        output = self.block(*args, **kwargs)\n",
    "        # print(output.shape)\n",
    "        # mask = self.mask if self.mask is not None else 1.0\n",
    "\n",
    "        if self.mod is not None:\n",
    "\n",
    "            if isinstance(self.pos[0], int):\n",
    "                assert len(self.mod.shape) == 3\n",
    "                if self.mod_method == \"add\":\n",
    "                    output[:, self.pos] += self.mod\n",
    "                elif self.mod_method == \"substitute\":\n",
    "                    output[:, self.pos] = self.mod\n",
    "                else:\n",
    "                    raise NotImplementedError(\"pos dimension not implemented\")\n",
    "                # self.modify(output[:, self.pos], self.mod)\n",
    "                if self.verbose:\n",
    "                    print(\"Modifying with broadcasting: pos:{} modshape:{}\".format(self.pos, self.mod.shape))\n",
    "\n",
    "            elif isinstance(self.pos[0], list) or isinstance(self.pos[0], torch.Tensor): # assume the modification differs for different elements in the batch\n",
    "                if self.verbose:\n",
    "                    print(\"Modifying iteratively: pos:{}\".format(self.pos))\n",
    "                assert len(self.pos) == len(self.mod)\n",
    "                for i in range(len(self.pos)):\n",
    "                    if self.mod_method == \"add\":\n",
    "                        output[i, self.pos] += self.mod[i]\n",
    "                    elif self.mod_method == \"substitute\":\n",
    "                        output[i, self.pos] = self.mod[i]\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"pos dimension not implemented\")\n",
    "                    # self.modify(output[:, self.pos], self.mod)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Indexing not accepted\")\n",
    "\n",
    "        return output\n",
    "\n",
    "    def set_mod(self, mod, pos, mod_method, verbose=False):\n",
    "        self.mod = mod\n",
    "        # self.mask = masks\n",
    "        self.pos = pos\n",
    "        self.mod_method = mod_method\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def reset(self):\n",
    "        self.output = None\n",
    "        self.mod = None\n",
    "        self.pos = None\n",
    "        self.verbose = False\n",
    "\n",
    "    def set_masks(self, masks):\n",
    "        self.mask = masks\n",
    "\n",
    "    \n",
    "class WrappedModModel(torch.nn.Module):\n",
    "    def __init__(self, model, tokenizer, verbose=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.verbose = verbose\n",
    "        if verbose:\n",
    "            print(\"Creating wrapped model\")\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "        \n",
    "    def generate(self, prompt, max_new_tokens=100, random_seed=0, use_cache=True):\n",
    "        with torch.no_grad():\n",
    "            torch.random.manual_seed(random_seed)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=512, truncation=True)\n",
    "            attention_mask = inputs.attention_mask.to(self.model.device)\n",
    "            generate_ids = self.model.generate(inputs.input_ids.to(self.model.device), attention_mask=attention_mask, max_new_tokens=max_new_tokens, use_cache=use_cache)\n",
    "            return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    def get_logits(self, tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(tokens.to(self.model.device)).logits\n",
    "            return logits\n",
    "        \n",
    "    def run_prompt(self, prompt, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=512, truncation=True)\n",
    "            input_ids = inputs.input_ids.to(self.model.device)\n",
    "            attention_mask = inputs.attention_mask.to(self.model.device)\n",
    "            output = self.model(input_ids, attention_mask=attention_mask)\n",
    "            return output\n",
    "        \n",
    "    def wrap_decoder_block(self, layer_id):\n",
    "        block = self.model.model.layers[layer_id]\n",
    "        if not self.is_wrapped(block):\n",
    "            self.model.model.layers[layer_id] = WrappedModBlock(block)\n",
    "    \n",
    "    def wrap_embedding_block(self):\n",
    "        block = self.model.model.embed_tokens\n",
    "        if not self.is_wrapped(block):\n",
    "            self.model.model.embed_tokens = WrappedModBlock(block)\n",
    "    \n",
    "    def wrap_all(self):\n",
    "        for layer_id, layer in enumerate(self.model.model.layers):\n",
    "            self.wrap_decoder_block(layer_id)\n",
    "            \n",
    "    def wrap_block(self, layer_ids):\n",
    "        def _wrap_block(layer_id):\n",
    "            self.wrap_decoder_block(layer_id)\n",
    "        if isinstance(layer_ids, list) or isinstance(layer_ids, tuple) or isinstance(layer_ids, np.ndarray):\n",
    "            for layer_id in layer_ids:\n",
    "                _wrap_block(layer_id)\n",
    "        else:\n",
    "            _wrap_block(layer_ids)\n",
    "    \n",
    "    def wrap_embedding_block(self):\n",
    "        block = self.model.model.embed_tokens\n",
    "        if not self.is_wrapped(block):\n",
    "            self.model.model.embed_tokens = WrappedModBlock(block)\n",
    "\n",
    "    def get_activations(self, layer_ids):\n",
    "\n",
    "        def _get_activations(layer_id):\n",
    "            current_layer = self.model.model.layers[layer_id]\n",
    "\n",
    "            if self.is_wrapped(current_layer):\n",
    "                current_block = current_layer.block\n",
    "                return current_layer.output\n",
    "                \n",
    "        if isinstance(layer_ids, list) or isinstance(layer_ids, tuple) or isinstance(layer_ids, np.ndarray):\n",
    "            activations = {}\n",
    "            for layer_id in layer_ids:\n",
    "                activations[layer_id] = _get_activations(layer_id)\n",
    "            return activations\n",
    "        else:\n",
    "            return _get_activations(layer_ids)\n",
    "\n",
    "    def set_mod(self, layer_ids, activations, block_name='decoder_block', pos=None, masks=None, normalize=False):\n",
    "\n",
    "        def _set_mod(layer_id, activations, block_name, masks, normalize):\n",
    "            current_layer = self.model.model.layers[layer_id]\n",
    "            current_layer.set_mod(activations, pos, masks, normalize)\n",
    "                \n",
    "        if isinstance(layer_ids, list) or isinstance(layer_ids, tuple) or isinstance(layer_ids, np.ndarray):\n",
    "            assert isinstance(activations, dict), \"activations should be a dictionary\"\n",
    "            for layer_id in layer_ids:\n",
    "                _set_mod(layer_id, activations[layer_id], block_name, masks, normalize)\n",
    "        else:\n",
    "            _set_mod(layer_ids, activations, block_name, masks, normalize)\n",
    "    \n",
    "    def set_mod_embedding(self, mod, pos, mod_method=\"substitute\"):\n",
    "        block = self.model.model.embed_tokens\n",
    "        assert self.is_wrapped(block)\n",
    "        block.set_mod(mod, pos, mod_method, verbose=self.verbose)\n",
    "        if self.verbose:\n",
    "            print(\"Setting embedding modification\")\n",
    "\n",
    "    def reset(self):\n",
    "        for layer in self.model.model.layers:\n",
    "            if self.is_wrapped(layer):\n",
    "                layer.reset()\n",
    "        self.model.model.embed_tokens.reset()\n",
    "\n",
    "    def set_masks(self, masks):\n",
    "        for layer in self.model.model.layers:\n",
    "            if self.is_wrapped(layer):\n",
    "                layer.set_masks(masks)\n",
    "\n",
    "    def is_wrapped(self, block):\n",
    "        if hasattr(block, 'block'):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def unwrap(self):\n",
    "        for l, layer in enumerate(self.model.model.layers):\n",
    "            if self.is_wrapped(layer):\n",
    "                self.model.model.layers[l] = layer.block\n",
    "        if self.is_wrapped(self.model.model.embed_tokens):\n",
    "            self.model.model.embed_tokens = model.model.embed_tokens.block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0051, -0.0053,  0.0044,  ..., -0.0010, -0.0022,  0.0003],\n",
      "         [-0.0051, -0.0053,  0.0044,  ..., -0.0010, -0.0022,  0.0003],\n",
      "         [-0.0051, -0.0053,  0.0044,  ..., -0.0010, -0.0022,  0.0003]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wrapped_model = WrappedModModel(model, tokenizer, verbose=False)\n",
    "wrapped_model.unwrap()\n",
    "wrapped_model.wrap_embedding_block()\n",
    "test_tokens = tokenizer(neutral_ds[0][0], return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "# wrapped_model(**test_tokens)\n",
    "init_embedding = get_seq_embedding(\"_ _ _\", tokenizer, model).unsqueeze(0)\n",
    "test_pos = [-1, -2, -3]\n",
    "\n",
    "wrapped_model.set_mod_embedding(init_embedding, test_pos)\n",
    "wrapped_model(**test_tokens)\n",
    "# get_hidden_reps(wrapped_model, tokenizer, neutral_ds[0][0])\n",
    "\n",
    "# wrapped_model.reset()\n",
    "get_hidden_reps(wrapped_model, tokenizer, neutral_ds[0][0], rep_tokens=[-1, -2])\n",
    "print(init_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_reps(wrapped_model: WrappedModModel, tokenizer, input_strs, sample_ids, mod, mod_pos, rep_tokens, grad_comp=True):\n",
    "    # Return hidden representation for (modified) inputs\n",
    "\n",
    "    wrapped_model.reset()\n",
    "    if mod is not None:\n",
    "        wrapped_model.set_mod_embedding(mod, mod_pos)\n",
    "    \n",
    "    tokenized_inputs = tokenizer(input_strs, return_tensors=\"pt\", padding=True).to(wrapped_model.model.device)\n",
    "    if not grad_comp:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_inputs, output_hidden_states=True)\n",
    "    else:\n",
    "        outputs = model(**tokenized_inputs, output_hidden_states=True)\n",
    "    reps = [m[:, rep_tokens] for m in outputs.hidden_states]\n",
    "    assert len(sample_ids) == reps[0].shape[0], \"sample_id passed incorrectly\"\n",
    "    \n",
    "    res = {sample_id: [x[i] for x in reps] for i, sample_id in enumerate(sample_ids)}\n",
    "    return res\n",
    "\n",
    "def comp_target_reps_interpolation(reps_init, reps_dest, dest_weight):\n",
    "    # Interpolate between reps_init and reps_dest\n",
    "    res = {}\n",
    "    for k in reps_init.keys():\n",
    "        l = []\n",
    "        for i in range(len(reps_init[k])):\n",
    "            l.append((reps_init[k][i] * (1 - dest_weight) + reps_dest[k][i] * dest_weight).detach())\n",
    "            # print(torch.norm(reps_init[k][i] - reps_dest[k][i]))\n",
    "        res[k] = l\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_rep_diff(base, target, layer_ids):\n",
    "    loss = 0\n",
    "    for l in layer_ids:\n",
    "        # print(base[l], target[l])\n",
    "        loss += torch.norm(base[l] - target[l])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2393,  0.0964,  0.0645,  ..., -0.1451,  0.2477, -0.0534],\n",
      "        [ 0.1218,  0.0615, -0.0446,  ..., -0.1791,  0.2061,  0.0233],\n",
      "        [ 0.0465,  0.0299, -0.2412,  ...,  0.1476,  0.3298, -0.1207]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[-0.1958, -0.1316,  0.0049,  ...,  0.3669,  0.0353,  0.1670],\n",
      "        [-0.1868, -0.0812, -0.0630,  ...,  0.2739,  0.0055,  0.1926],\n",
      "        [-0.1556, -0.0936, -0.0111,  ...,  0.3472,  0.0879,  0.1354]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "def train_contrastive(model, tokenizer, ds_dest, init_str, target_str, epochs, batchsize, rep_tokens, coef=1):\n",
    "\n",
    "    cached_target_reps = {}\n",
    "\n",
    "    # Created wrapped model for injection\n",
    "    wrapped_model = WrappedModModel(model, tokenizer, verbose=False)\n",
    "    wrapped_model.unwrap()\n",
    "    wrapped_model.wrap_embedding_block()\n",
    "    wrapped_model.reset()\n",
    "\n",
    "    embedding_init = get_seq_embedding(init_str, tokenizer, wrapped_model.model)\n",
    "    # print(embedding_init)\n",
    "\n",
    "    var_embedding = embedding_init.clone()\n",
    "    # var_embedding.requires_grad = True\n",
    "\n",
    "    for i in range(epochs):\n",
    "        for batch_ind in range((len(ds_dest) + batchsize - 1) // batchsize)[:2]:\n",
    "            \n",
    "            # Compute or retrive target representation\n",
    "            sample_id_ls = [] # all sample ids\n",
    "            sample_id_ls_target_recomp = [] # ids not found in cache\n",
    "\n",
    "            input_strs, mod_pos_ls = [], []\n",
    "            input_strs_target_recomp, mod_pos_ls_target_recomp = [], []\n",
    "            \n",
    "            for target_str, mod_pos, sample_id in ds_dest[batch_ind * batchsize : (batch_ind + 1) * batchsize]:\n",
    "                \n",
    "                # sample_id: (i, j) - the j-th truncation for the i-th sentence\n",
    "                sample_id_ls.append(sample_id)\n",
    "                input_strs.append(target_str)\n",
    "                mod_pos_ls.append(mod_pos)\n",
    "                \n",
    "                if sample_id not in cached_target_reps:\n",
    "                    sample_id_ls_target_recomp.append(sample_id)\n",
    "                    input_strs_target_recomp.append(target_str)\n",
    "                    mod_pos_ls_target_recomp.append(mod_pos)\n",
    "                \n",
    "            # Compute reps at init and target token\n",
    "            if len(sample_id_ls_target_recomp) != 0:\n",
    "                # Recompute\n",
    "                reps_init_recomp = comp_reps(\n",
    "                    wrapped_model,\n",
    "                    tokenizer,\n",
    "                    input_strs_target_recomp,\n",
    "                    sample_id_ls_target_recomp,\n",
    "                    mod=[embedding_init.clone() for i in range(len(sample_id_ls_target_recomp))],\n",
    "                    mod_pos=mod_pos_ls_target_recomp,\n",
    "                    rep_tokens=rep_tokens,\n",
    "                    grad_comp=False\n",
    "                )\n",
    "\n",
    "                reps_dest_recomp = comp_reps(\n",
    "                    wrapped_model,\n",
    "                    tokenizer,\n",
    "                    input_strs_target_recomp,\n",
    "                    sample_id_ls_target_recomp,\n",
    "                    mod=None,\n",
    "                    mod_pos=None,\n",
    "                    rep_tokens=rep_tokens,\n",
    "                    grad_comp=False\n",
    "                )\n",
    "\n",
    "                reps_target_recomp = comp_target_reps_interpolation(reps_init_recomp, reps_dest_recomp, coef)\n",
    "                cached_target_reps.update(reps_target_recomp)\n",
    "\n",
    "            reps_target = {sample_id: cached_target_reps[sample_id] for sample_id in sample_id_ls}\n",
    "            # Compute the representation with the varying embedding\n",
    "\n",
    "            # Create the varying embedding representations\n",
    "            var_embedding_dups = [var_embedding.clone() for i in range(len(sample_id_ls))]\n",
    "            for x in var_embedding_dups:\n",
    "                x.requires_grad = True\n",
    "\n",
    "            reps_var = comp_reps(\n",
    "                wrapped_model,\n",
    "                tokenizer,\n",
    "                input_strs,\n",
    "                sample_id_ls,\n",
    "                mod=var_embedding_dups,\n",
    "                mod_pos=mod_pos_ls,\n",
    "                rep_tokens=rep_tokens,\n",
    "                grad_comp=True\n",
    "            )\n",
    "\n",
    "            # print(var_embedding_dups)\n",
    "\n",
    "            # Compute representation difference in l2 metric\n",
    "            losses = {}\n",
    "            for sample_id in reps_var:\n",
    "                losses[sample_id] = l2_rep_diff(reps_var[sample_id], reps_target[sample_id], [-5])\n",
    "            \n",
    "            # Back prop the loss to modified embeddings\n",
    "            total_loss = 0\n",
    "            for sample_id in losses:\n",
    "                total_loss += losses[sample_id]\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # for i in range(len(var_embedding_dups)):\n",
    "            #     print(i, var_embedding_dups[i].grad)\n",
    "\n",
    "            # tokenized_var_strs = tokenizer(var_strs, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "            # print(model(**tokenized_var_strs))\n",
    "            ave_grad = torch.mean(torch.stack([var_embedding_dups[i].grad for i in range(len(var_embedding_dups))]), dim=0)\n",
    "            print(ave_grad)\n",
    "\n",
    "            # TODO: Complete gradient update\n",
    "\n",
    "\n",
    "res = train_contrastive(model, tokenizer, target_ds, aug_neutral, aug_target, epochs=1, batchsize=16, rep_tokens=[-1], coef=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
